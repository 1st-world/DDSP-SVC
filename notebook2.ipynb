{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 라이브러리 로드 및 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from logger import utils\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "from logger.utils import traverse_dir\n",
    "\n",
    "# Cuda setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Configure loading\n",
    "args = utils.load_config('./configs/reflow.yaml')\n",
    "\n",
    "# Set path\n",
    "MP4_DATA_PATH   = 'preprocess/mp4'\n",
    "ORIGINAL_PATH   = 'preprocess/original/'\n",
    "DEMUCS_PATH     = 'preprocess/demucs/'\n",
    "NORM_PATH       = 'preprocess/norm/'\n",
    "TEMP_LOG_PATH   = 'temp_ffmpeg_log.txt' # 무음 감지 로그 임시 저장 위치"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비\n",
    "***\n",
    "1. 전처리가 불필요한 경우 (모든 데이터가 배경음이 없고 특정 길이로 잘려 있는 경우),\n",
    "    - 모든 데이터를 다음과 같이 data/train/audio 경로에 삽입하고\n",
    "        ```\n",
    "        # training dataset\n",
    "        data/train/audio/aaa.wav\n",
    "        data/train/audio/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1-6. Validation 파일 분할 단계로 건너뛰기\n",
    "***\n",
    "2. 배경음은 제거되었지만 데이터가 너무 길어서 특정 길이로 잘라야 하는 경우,\n",
    "    - 모든 데이터를 다음과 같이 preprocess/norm 경로에 삽입하고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/norm/aaa.wav\n",
    "        preprocess/norm/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1-4. Split 단계로 건너뛰기\n",
    "***\n",
    "3. 아무 처리도 되어 있지 않은 경우 (배경음도 제거해야 하고 데이터도 잘라야 하는 경우),\n",
    "    - 모든 데이터를 다음과 같이 preprocess/original 경로에 삽입하고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/original/aaa.wav\n",
    "        preprocess/original/bbb.wav\n",
    "        ...\n",
    "        ```\n",
    "    - 1-2. Demucs 단계로 건너뛰기\n",
    "***\n",
    "4. 아무 처리도 되어 있지 않고 mp4 파일일 경우,\n",
    "    - 모든 데이터를 다음과 같이 preprocess/mp4 경로에 삽입하고\n",
    "        ```\n",
    "        # training dataset\n",
    "        preprocess/mp4/aaa.mp4\n",
    "        preprocess/mp4/bbb.mp4\n",
    "        ...\n",
    "        ```\n",
    "    - 1-1. mp4 데이터 변환 단계부터 차례대로 진행\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. mp4 데이터를 wav로 변환\n",
    "- preprocess/mp4 경로에 있는 mp4 파일을 wav로 변환 후 preprocess/original 경로에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp4_to_wav(input_dir: str, input_file: str, output_dir: str):\n",
    "    \"\"\" mp4 파일을 wav 형식으로 변환합니다.\n",
    "    args:\n",
    "        input_dir (str): 입력 mp4 파일의 경로\n",
    "        input_file (str): 입력 mp4 파일의 이름\n",
    "        output_dir (str): 출력 wav 파일의 경로\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(input_file)[1][1:]\n",
    "\n",
    "    if ext is not \"mp4\":\n",
    "        return \n",
    "    else:\n",
    "        track = AudioSegment.from_file(os.path.join(input_dir,input_file), format='mp4')\n",
    "        track = track.set_frame_rate(44100)\n",
    "        track.export(os.path.join(output_dir, input_file[:-4]+\".wav\"), format='wav')\n",
    "\n",
    "filelist = traverse_dir(\n",
    "    MP4_DATA_PATH,\n",
    "    extension='mp4',\n",
    "    is_pure=True,\n",
    "    is_sort=True,\n",
    "    is_ext=True)\n",
    "\n",
    "for file in tqdm(filelist, desc=\"변환 중 \"):\n",
    "    mp4_to_wav(MP4_DATA_PATH, file, ORIGINAL_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Demucs (배경음 제거)\n",
    "- preprocess/original 경로에 있는 wav 파일의 배경음(멜로디)을 제거 후 preprocess/demucs 경로에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sep_wav import demucs\n",
    "\n",
    "demucs(ORIGINAL_PATH, DEMUCS_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Normalize\n",
    "- preprocess/demucs 경로에 있는 배경음이 제거된 데이터를 노멀라이즈 (sample rate 값 조정 가능) 후 preprocess/norm 경로에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sep_wav import audio_norm\n",
    "\n",
    "for filepath in tqdm(glob(DEMUCS_PATH+\"*.wav\"), desc=\"Normalizing... \"):\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(NORM_PATH, filename) + \".wav\"\n",
    "    audio_norm(filepath, out_filepath, sample_rate = 44100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Split\n",
    "- preprocess/norm 경로에 있는 데이터를 15초 길이로 자른 후 data/train/audio 경로에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "for filepath in tqdm(glob(NORM_PATH+\"*.wav\"), desc=\"Cutting... \"):\n",
    "    duration = librosa.get_duration(filename=filepath)\n",
    "    max_last_seg_duration = 0\n",
    "    sep_duration_final = 15\n",
    "    sep_duration = 15\n",
    "\n",
    "    while sep_duration > 4:\n",
    "        last_seg_duration = duration % sep_duration\n",
    "        if max_last_seg_duration < last_seg_duration:\n",
    "            max_last_seg_duration = last_seg_duration\n",
    "            sep_duration_final = sep_duration\n",
    "        sep_duration -= 1\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    out_filepath = os.path.join(args.data.train_path,\"audio\", f\"{filename}-%04d.wav\")\n",
    "    subprocess.run(f'ffmpeg -i \"{filepath}\" -f segment -segment_time {sep_duration_final} \"{out_filepath}\" -y', capture_output=True, shell=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5. 무음 제거\n",
    "- data/train/audio 경로에 있는 음원들 중 무음(無音)인 파일을 찾아 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sep_wav import get_ffmpeg_args\n",
    "import subprocess\n",
    "\n",
    "for filepath in tqdm(glob(args.data.train_path+\"/audio/*.wav\"), desc=\"Removing... \"):\n",
    "    if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)\n",
    "\n",
    "    ffmpeg_arg = get_ffmpeg_args(filepath)\n",
    "    subprocess.run(ffmpeg_arg, capture_output=True, shell=True)\n",
    "\n",
    "    start = None\n",
    "    end = None\n",
    "\n",
    "    with open(TEMP_LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if \"lavfi.silence_start\" in line:\n",
    "                start = float(line.split(\"=\")[1])\n",
    "            if \"lavfi.silence_end\" in line:\n",
    "                end = float(line.split(\"=\")[1])\n",
    "\n",
    "    if start != None:\n",
    "        if start == 0 and end == None:\n",
    "            os.remove(filepath)\n",
    "            \n",
    "if os.path.exists(TEMP_LOG_PATH):\n",
    "        os.remove(TEMP_LOG_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-6. Validation 파일 분할\n",
    "- data/train/audio 경로에 있는 데이터 중 일정한 비율 만큼을 나누어 data/val/audio 경로로 이동\n",
    "    - 계산식: `max(SAMPLE_MIN, min(SAMPLE_MAX, int(len(all_files) * ratio)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from draw import main\n",
    "\n",
    "# Configure setting\n",
    "configures = {\n",
    "    # Optional : Default values are available\n",
    "    'train'         :   os.path.abspath('.') + \"/data/train/audio\",\n",
    "    'val'           :   os.path.abspath('.') + \"/data/val/audio\",\n",
    "    'sample_rate'   :   1,\n",
    "    'extensions'    :   [\"wav\", \"flac\"]\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "main(cmd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 학습을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Encoder Model] Content Vec\n",
      " [Loading] pretrain/contentvec/checkpoint_best_legacy_500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 19:32:09 | INFO | fairseq.tasks.hubert_pretraining | current directory is c:\\Users\\PARK\\Desktop\\Project\\DDSP-SVC\n",
      "2024-07-17 19:32:09 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'metadata', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-07-17 19:32:09 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "c:\\Users\\PARK\\Desktop\\Project\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the audio clips in : data/train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/92 [00:00<?, ?it/s]c:\\Users\\PARK\\Desktop\\Project\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "100%|██████████| 92/92 [00:21<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess the audio clips in : data/val\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocess\n",
    "from ddsp.vocoder import F0_Extractor, Volume_Extractor, Units_Encoder\n",
    "from diffusion.vocoder import Vocoder\n",
    "\n",
    "# Get data\n",
    "sample_rate = args.data.sampling_rate\n",
    "hop_size = args.data.block_size\n",
    "extensions = args.data.extensions\n",
    "\n",
    "# Initialize f0 extractor\n",
    "f0_extractor = F0_Extractor(\n",
    "                    args.data.f0_extractor, \n",
    "                    args.data.sampling_rate, \n",
    "                    args.data.block_size, \n",
    "                    args.data.f0_min, \n",
    "                    args.data.f0_max)\n",
    "\n",
    "# Initialize volume extractor\n",
    "volume_extractor = Volume_Extractor(args.data.block_size)\n",
    "\n",
    "# Initialize mel extractor\n",
    "mel_extractor = None\n",
    "use_pitch_aug = False\n",
    "if args.model.type in ['Diffusion', 'DiffusionNew', 'DiffusionFast', 'RectifiedFlow']:\n",
    "    mel_extractor = Vocoder(args.vocoder.type, args.vocoder.ckpt, device = device)\n",
    "    if mel_extractor.vocoder_sample_rate != sample_rate or mel_extractor.vocoder_hop_size != hop_size:\n",
    "        mel_extractor = None\n",
    "        print(\"Unmatch vocoder parameters, mel extraction is ignored!\")\n",
    "    elif args.model.use_pitch_aug:\n",
    "        use_pitch_aug = True\n",
    "\n",
    "# Initialize units encoder\n",
    "if args.data.encoder == 'cnhubertsoftfish':\n",
    "        cnhubertsoft_gate = args.data.cnhubertsoft_gate\n",
    "else:\n",
    "    cnhubertsoft_gate = 10\n",
    "units_encoder = Units_Encoder(\n",
    "                    args.data.encoder, \n",
    "                    args.data.encoder_ckpt, \n",
    "                    args.data.encoder_sample_rate, \n",
    "                    args.data.encoder_hop_size,\n",
    "                    cnhubertsoft_gate=cnhubertsoft_gate,\n",
    "                    device = device)\n",
    "\n",
    "# Preprocess training set\n",
    "preprocess(args.data.train_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device, use_pitch_aug = use_pitch_aug, extensions = extensions)\n",
    "\n",
    "# Preprocess validation set\n",
    "preprocess(args.data.valid_path, f0_extractor, volume_extractor, mel_extractor, units_encoder, sample_rate, hop_size, device = device, use_pitch_aug = use_pitch_aug, extensions = extensions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_reflow import training\n",
    "\n",
    "training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Old version : Use `combsub.yaml` as config\n",
    "from train import training\n",
    "\n",
    "training(args)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 결과물 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from main_reflow import inference\n",
    "\n",
    "# Configure setting\n",
    "configures = {\n",
    "    # Values input Required\n",
    "    'model_ckpt'        :   'exp/reflow-test/model_00000.pt',   # 모델 체크포인트 경로\n",
    "    'input'             :   'data/train/audio/name_1234.wav',   # 원본 노래 파일 경로\n",
    "    'output'            :   'output.wav',                       # 출력 파일 저장할 경로\n",
    "    # Optional : Default values are available\n",
    "    'device'            :   device,\n",
    "    'spk_id'            :   '1',\n",
    "    'spk_mix_dict'      :   'None',\n",
    "    'key'               :   '0',\n",
    "    'formant_shift_key' :   '0',\n",
    "    'pitch_extractor'   :   'rmvpe',\n",
    "    'f0_min'            :   '50',\n",
    "    'f0_max'            :   '1100',\n",
    "    'threhold'          :   '-60',\n",
    "    'infer_step'        :   'auto',\n",
    "    'method'            :   'auto',\n",
    "    't_start'           :   'auto'\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "inference(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Old version : Use `combsub.yaml` as config\n",
    "from types import SimpleNamespace\n",
    "from main import inference\n",
    "\n",
    "# Configure setting\n",
    "configures = {\n",
    "    # Values input Required\n",
    "    'model_path'            :   'exp/combsub-test/model_00000.pt',  # 모델 체크포인트 경로\n",
    "    'input'                 :   'data/train/audio/name_1234.wav',   # 원본 노래 파일 경로\n",
    "    'output'                :   'output.wav',                       # 출력 파일 저장할 경로\n",
    "    # Optional : Default values are available\n",
    "    'device'                :   device,\n",
    "    'spk_id'                :   '1',\n",
    "    'spk_mix_dict'          :   'None',\n",
    "    'key'                   :   '0',\n",
    "    'enhance'               :   'true',\n",
    "    'pitch_extractor'       :   'rmvpe',\n",
    "    'f0_min'                :   '50',\n",
    "    'f0_max'                :   '1100',\n",
    "    'threhold'              :   '-60',\n",
    "    'enhancer_adaptive_key' :   '0'\n",
    "}\n",
    "cmd = SimpleNamespace(**configures)\n",
    "\n",
    "inference(cmd)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a643f5fe528358e1cfac3836870fd104c9c787e6f994a831162d9d1f5f0281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
